{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "StackExachange_tags_part2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aks2093/StackExchange-Question-Tags-Prediction/blob/master/StackExachange_tags_part2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1JuhYA7HkIG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "94caa0a5-24e4-46b9-d9aa-33e87b035b8c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJogxvixZDvb",
        "colab_type": "text"
      },
      "source": [
        "# Post Proressing of the output tags if it contains \"rare\" tag"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8qrhR9JZOkA",
        "colab_type": "text"
      },
      "source": [
        "**read the data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_yInW8KIfrM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "fd01c3bc-9fca-4768-faae-28eb0efa6dcb"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "DATA_DIR = \"/content/drive/My Drive/stackExchange_tag_predictions/data\"\n",
        "if os.path.isfile(os.path.join(DATA_DIR, \"combined_data.csv\")):\n",
        "    combined_data = pd.read_csv(os.path.join(DATA_DIR, \"combined_data.csv\"))\n",
        "    print(\"combined Data read\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "combined Data read\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RB42P4uZYP3",
        "colab_type": "text"
      },
      "source": [
        "**Get total tags and unique tags**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gy9lNQvWHrnY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "a8dc4bf4-a74b-42ba-f99f-9d877c3be53c"
      },
      "source": [
        "Y = combined_data[[\"tags\"]]\n",
        "\n",
        "Y[\"tags\"] = Y[\"tags\"].apply(lambda x: x.lower().split(\" \"))\n",
        "print(Y.head(5))\n",
        "unique_tags = []\n",
        "total_tags = []\n",
        "for _, row in Y.iterrows():\n",
        "    total_tags.extend(list(row[\"tags\"]))\n",
        "    if len(unique_tags) == 0:\n",
        "        unique_tags = set(row[\"tags\"])\n",
        "    else:\n",
        "        unique_tags = unique_tags.union(set(row[\"tags\"]))\n",
        "\n",
        "number_of_unique_tags = len(unique_tags)\n",
        "print(\"Total number of unique tags: {}\".format(number_of_unique_tags))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                                   tags\n",
            "0      [aes, pseudo-random-permutation]\n",
            "1           [junction-box, low-voltage]\n",
            "2  [cryptanalysis, substitution-cipher]\n",
            "3            [motor, quadcopter, power]\n",
            "4                  [electrical, boiler]\n",
            "Total number of unique tags: 4268\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9mMkyQ0KFbU",
        "colab_type": "text"
      },
      "source": [
        "# Build a probabilistic matrix\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvlMgdkA5LcD",
        "colab_type": "text"
      },
      "source": [
        "Building a Event Matrix whose rows will represent frequent_tags and columns will represent rare_tags. \n",
        "\n",
        "value to be filled out in this matrix would be\n",
        "event_matrix[frequent_tag][rare_tag]= Number of events when frequent_tag and rare_tag occured together\n",
        "\n",
        "Idea is to compute the probability vector for each rare_tag so that we can associate them with frequent_tag.\n",
        "\n",
        "**Proposed Algorithm:**\n",
        "\n",
        "\n",
        "> for all rare_tags:\n",
        "\n",
        "1.   Compute P(rare_tag|freq_tag1) X P(rare_tag|freq_tag2) X........X P(rare_tag|freq_tag_n)\n",
        "2.   add them to probability vector\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**The Top K values in the probability vector will give the K rare_tags that can be associated with frequent_tag that was predicted by model.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAOpqMVUQedo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "counter_dict = {}\n",
        "for tag in total_tags:\n",
        "    if tag not in counter_dict:\n",
        "        counter_dict[tag] = 1\n",
        "    else:\n",
        "        counter_dict[tag] += 1\n",
        "\n",
        "rare_tags = []\n",
        "frequent_tags = []\n",
        "\n",
        "for key, val in counter_dict.items():\n",
        "    if val<=500:\n",
        "        rare_tags.append(key)\n",
        "    else:\n",
        "        frequent_tags.append(key)\n",
        "\n",
        "row_wise_tags = []\n",
        "for _, row in Y.iterrows():\n",
        "    row_wise_tags.append(list(row[\"tags\"]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M788yR6vJvKs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "7627a414-2ccc-42fd-c387-e2e8db4b34ea"
      },
      "source": [
        "print(\"Number of rare tags: {}\".format(len(rare_tags)))\n",
        "print(\"Number of frequent tags: {}\".format(len(frequent_tags)))\n",
        "print(\"row_wise_tags length: {}\".format(len(row_wise_tags)))\n",
        "#Build Event Matrix\n",
        "print(\"Computing Event Matrix.....\")\n",
        "event_matrix = [[0]*len(rare_tags) for i in range(len(frequent_tags))]\n",
        "# numpy.zeros(shape=(len(frequent_tags),len(rare_tags)))\n",
        "\n",
        "print(\"num of rows in event matrix: {}\".format(len(event_matrix)))\n",
        "print(\"num of cols in event matrix: {}\".format(len(event_matrix[0])))\n",
        "print(\"Initialization of Event Matrix Done.....\")\n",
        "\n",
        "for rare_tag_idx in range(len(rare_tags)):\n",
        "    for frequent_tag_idx in range(len(frequent_tags)):\n",
        "        count = 0\n",
        "        for row_tags in row_wise_tags:\n",
        "            if (rare_tags[rare_tag_idx] in row_tags) and (frequent_tags[frequent_tag_idx] in row_tags):\n",
        "                count += 1\n",
        "        event_matrix[frequent_tag_idx][rare_tag_idx] = count\n",
        "\n",
        "event_matrix = np.array(event_matrix)\n",
        "np.save(os.path.join(DATA_DIR, \"event_matrix\"), event_matrix)\n",
        "\n",
        "print(\"Event Matrix computed\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of rare tags: 4203\n",
            "Number of frequent tags: 65\n",
            "row_wise_tags length: 87000\n",
            "Computing Event Matrix.....\n",
            "num of rows in event matrix: 65\n",
            "num of cols in event matrix: 4203\n",
            "Initialization of Event Matrix Done.....\n",
            "Event Matrix computed\n",
            "['communication', 'laundry', 'pregnancy', 'sex', 'primer']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDnEE4YtP-P3",
        "colab_type": "text"
      },
      "source": [
        "**Make Prediction based on Probability model for rare_tags**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xN_Uk1YpP-qD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "a497295b-f820-4384-ff75-73711489bc4b"
      },
      "source": [
        "import numpy as np\n",
        "event_matrix = np.load(os.path.join(DATA_DIR, \"event_matrix.npy\"))\n",
        "\n",
        "#Get the rare_tags based on probability\n",
        "def get_rare_tags(tags, top_k=5):\n",
        "    if \"rare\" in tags and len(tags)>=2:\n",
        "        tags.remove(\"rare\")\n",
        "        probability_vector = []\n",
        "        for rare_tag in rare_tags:\n",
        "            p = 1.0\n",
        "            for tag in tags:\n",
        "                rare_tag_idx = rare_tags.index(rare_tag)\n",
        "                tag_idx = frequent_tags.index(tag)\n",
        "                if event_matrix[tag_idx][rare_tag_idx] != 0:\n",
        "                    p = p*(event_matrix[tag_idx][rare_tag_idx]/counter_dict[tag])\n",
        "            if p<1.0:\n",
        "                probability_vector.append((rare_tag, p))\n",
        "\n",
        "        probability_vector.sort(key=lambda x : x[1])\n",
        "        predicted_rare_tags = []\n",
        "        for i in range(top_k):\n",
        "            predicted_rare_tags.append(probability_vector[i][0])\n",
        "\n",
        "        return predicted_rare_tags\n",
        "    else:\n",
        "        raise Exception(\"No rare tag found\")\n",
        "    \n",
        "print(\"Predicted top 5 rare_tags are: \")\n",
        "print(get_rare_tags([\"evolution\", \"genetics\", \"molecular-biology\", \"rare\"]))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted top 5 rare_tags are: \n",
            "['extremophiles', 'protein-folding', 'network', 'invertebrates', 'radiation']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXfW2Sv9XVFu",
        "colab_type": "text"
      },
      "source": [
        "**Define custom functions those were used while training the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-r9gsOeyT98h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import backend as K\n",
        "import tensorflow\n",
        "\n",
        "def wbce( y_true, y_pred, weight1=0.9, weight0=0.1 ) :\n",
        "    y_true = K.clip(y_true, K.epsilon(), 1-K.epsilon())\n",
        "    y_pred = K.clip(y_pred, K.epsilon(), 1-K.epsilon())\n",
        "    logloss = -(y_true * K.log(y_pred) * weight1 + (1 - y_true) * K.log(1 - y_pred) * weight0 )\n",
        "    return K.mean( logloss, axis=-1)\n",
        "\n",
        "def micro_f1_score(y_true, y_pred, threshold=None):\n",
        "    def recall(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "        return recall\n",
        "\n",
        "    def precision(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "    \n",
        "    if threshold is None:\n",
        "        threshold = tensorflow.reduce_max(y_pred, axis=-1, keepdims=True)\n",
        "        y_pred = tensorflow.logical_and(y_pred >= threshold, tensorflow.abs(y_pred) > 1e-12)\n",
        "    else:\n",
        "        y_pred = y_pred > threshold\n",
        "    \n",
        "    y_true = tensorflow.cast(y_true, tensorflow.float32)\n",
        "    y_pred = tensorflow.cast(y_pred, tensorflow.float32)\n",
        "\n",
        "    precision = precision(y_true, y_pred)\n",
        "    recall = recall(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "\n",
        "def jaccard_distance(y_true, y_pred, threshold=None, smooth=100):\n",
        "    if threshold is None:\n",
        "        threshold = tensorflow.reduce_max(y_pred, axis=-1, keepdims=True)\n",
        "        y_pred = tensorflow.logical_and(y_pred >= threshold, tensorflow.abs(y_pred) > 1e-12)\n",
        "    else:\n",
        "        y_pred = y_pred > threshold\n",
        "    \n",
        "    y_true = tensorflow.cast(y_true, tensorflow.int32)\n",
        "    y_pred = tensorflow.cast(y_pred, tensorflow.int32)\n",
        "\n",
        "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
        "    sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\n",
        "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
        "    return (1 - jac)\n",
        "\n",
        "def hamming_loss(y_true, y_pred, threshold=None):\n",
        "    if threshold is None:\n",
        "        threshold = tensorflow.reduce_max(y_pred, axis=-1, keepdims=True)\n",
        "        y_pred = tensorflow.logical_and(y_pred >= threshold, tensorflow.abs(y_pred) > 1e-12)\n",
        "    else:\n",
        "        y_pred = y_pred > threshold\n",
        "\n",
        "    y_true = tensorflow.cast(y_true, tensorflow.int32)\n",
        "    y_pred = tensorflow.cast(y_pred, tensorflow.int32)\n",
        "\n",
        "    nonzero = tensorflow.cast(tensorflow.math.count_nonzero(y_true - y_pred, axis=-1), tensorflow.float32)\n",
        "    return nonzero / y_true.get_shape()[-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ns-zXONZQ6sr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "d1c133b3-7cef-4cba-a3b5-a75f85da9d40"
      },
      "source": [
        "from tensorflow import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "X = combined_data[[\"title_content\"]]\n",
        "Y = combined_data[[\"tags\"]]\n",
        "Y[\"tags\"] = Y[\"tags\"].apply(lambda x: x.lower().split(\" \"))\n",
        "\n",
        "counter_dict = {}\n",
        "for tag in total_tags:\n",
        "    if tag not in counter_dict:\n",
        "        counter_dict[tag] = 1\n",
        "    else:\n",
        "        counter_dict[tag] += 1\n",
        "\n",
        "rare_tags = []\n",
        "for key, val in counter_dict.items():\n",
        "    if val<=500:\n",
        "        rare_tags.append(key)\n",
        "\n",
        "from copy import deepcopy\n",
        "def remove_rare_tags(tags):\n",
        "    temp_tags = deepcopy(tags)\n",
        "    is_rare = False\n",
        "    for tag in temp_tags:\n",
        "        if tag in rare_tags:\n",
        "            is_rare = True\n",
        "            tags.remove(tag)\n",
        "    if is_rare:\n",
        "        tags.append(\"rare\")\n",
        "    return tags\n",
        "\n",
        "print(\"Number of rare tags seen: {}\".format(len(rare_tags)))\n",
        "\n",
        "Y[\"tags\"] = Y[\"tags\"].apply(remove_rare_tags)\n",
        "mlb = MultiLabelBinarizer()\n",
        "Y_bin = mlb.fit_transform(Y[\"tags\"])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of rare tags seen: 4203\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:36: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4NMROKIeiWr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "24160d45-227f-44da-a525-878a0db6c41a"
      },
      "source": [
        "tokenizer_obj = Tokenizer()\n",
        "tokenizer_obj.fit_on_texts(X[\"title_content\"])\n",
        "max_length = max([len(s.split()) for s in X[\"title_content\"]])\n",
        "vocab_size = len(tokenizer_obj.word_index)+1\n",
        "print(\"vocab_size: {}\".format(vocab_size))\n",
        "title_content_tokens = tokenizer_obj.texts_to_sequences(X[\"title_content\"])\n",
        "title_content_token_len = [len(i) for i in title_content_tokens]\n",
        "title_content_tokens_pad = pad_sequences(title_content_tokens, maxlen=max_length, padding='pre')\n",
        "\n",
        "title_content_tokens_pad_sample = title_content_tokens_pad[45:55]\n",
        "model = keras.models.load_model(os.path.join(DATA_DIR,\"stackExchange_tags_model_Adam.h5\"),custom_objects={'wbce':wbce,\"hamming_loss\":hamming_loss,\"micro_f1_score\":micro_f1_score,\"jaccard_distance\":jaccard_distance })\n",
        "predictions = model.predict([title_content_tokens_pad_sample])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab_size: 167055\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhxpjafAo1-N",
        "colab_type": "text"
      },
      "source": [
        "**get labels based on a given optimal threshold**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJ0_WJ_AamYb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_labels_tf(y_pred, threshold=None):\n",
        "    y_pred = tensorflow.convert_to_tensor(y_pred)\n",
        "    if threshold is None:\n",
        "        threshold = tensorflow.reduce_max(y_pred, axis=-1, keepdims=True)\n",
        "        y_pred = tensorflow.logical_and(y_pred >= threshold, tensorflow.abs(y_pred) > 1e-12)\n",
        "    else:\n",
        "        y_pred = y_pred > threshold\n",
        "    return y_pred\n",
        "\n",
        "def get_labels_np(y_pred, threshold=0.5):\n",
        "    if threshold is None:\n",
        "        threshold = np.amax(y_pred, axis=-1, keepdims=True)\n",
        "        y_pred = np.logical_and(y_pred >= threshold, np.absolute(y_pred) > 1e-12)\n",
        "    else:\n",
        "        y_pred = y_pred > threshold\n",
        "    return y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYJztwhC2rv-",
        "colab_type": "text"
      },
      "source": [
        "# Results\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Q1LyrpX44RB",
        "colab_type": "text"
      },
      "source": [
        "The current trained model is giving following results:\n",
        "\n",
        "**Train:**\n",
        "micro_f1_score: 0.6793 - hamming_loss: 0.0129 - jaccard_distance: 0.0083 \n",
        "\n",
        "**Test:**\n",
        "val_micro_f1_score: 0.6787 - val_hamming_loss: 0.0129 - val_jaccard_distance: 0.0083"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFTGoRN0yWCn",
        "colab_type": "text"
      },
      "source": [
        "# Future Work\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2ra9Ed247IZ",
        "colab_type": "text"
      },
      "source": [
        "As we have seen that the target tags were quite sparse(shape=87000x4268) because of rare tags(frequency<=500) so Classification algorithm was not able to perform well on these many tags(labels). So we had to make a separate new category for all rare tags(count=4203) so that dimentions of the target gets reduced and resultant target matrix would be less sparse so that out algo can I dentify each label appropriately.\n",
        "\n",
        "Although we can further improve the results with following few approaches:\n",
        "1. Making a proper bucketing of rare tags based on the correlation exist among them annd then train the Deep learning model on that.\n",
        "2. Take the data encodings from sencond last layer from the trained model and build a separate model that will predict those rare tags.\n",
        "3. We can also predict the tags based on the correlation among them\n",
        "4. skip-gram model can help to predict the rare tags based on similar context shared.\n",
        "\n",
        "We can also use the some advanced pretrained models like **BERT** or **T5** and predict the tags based on the encodings coming from these pretrained models. Due to time constraint I cound not use these models. I would like to share another notebook based on results of these advanced models\n",
        "\n",
        "One more nobel approach I found that reddit follows to classify its post to subreddits. "
      ]
    }
  ]
}